{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "    <a href=\"https://colab.research.google.com/github/quamernasim/YOLO-Wrold-See-Beyond-Labels/blob/main/yolo-world.ipynb\" target=\"_parent\">\n",
        "        <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "    </a>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Install the Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gag7p5pX6hRH"
      },
      "source": [
        "- https://arxiv.org/pdf/2401.17270.pdf\n",
        "- https://github.com/AILab-CVC/YOLO-World\n",
        "- https://huggingface.co/spaces/stevengrove/YOLO-World\n",
        "- https://colab.research.google.com/drive/1AmhbXBmH2MnJA8_aQ5EDoormQ61xlzdp#scrollTo=YQ86d81wVGMe\n",
        "- https://twitter.com/skalskip92/status/1754916529672438173"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uVAOjujV6hRK"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/onuralpszr/mmyolo.git -b version/mmcv\n",
        "%cd mmyolo/\n",
        "!git branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYetPhdY6hRN"
      },
      "outputs": [],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAAE28t46hRN"
      },
      "outputs": [],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToyYDKbt6hRO"
      },
      "outputs": [],
      "source": [
        "!git clone --recursive https://github.com/onuralpszr/YOLO-World.git -b collab_friendly\n",
        "%cd YOLO-World/\n",
        "!git branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GDSfoZmE6hRO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Install certain version of requests,tqdm,rich for openxlab (fix for yolo_world)\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "  !pip install requests==2.28.2 tqdm==4.65.0 rich==13.4.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJUbaxQW6hRP"
      },
      "outputs": [],
      "source": [
        "!python setup.py build develop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRYQB-sB6hRQ"
      },
      "outputs": [],
      "source": [
        "%pip install -U openmim\n",
        "!mim install \"mmengine>=0.7.0\"\n",
        "!mim install \"mmcv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dRvA2p1F6hRQ"
      },
      "outputs": [],
      "source": [
        "!pip install supervision==0.18.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Icw5FTHo6hRR"
      },
      "outputs": [],
      "source": [
        "quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting started with Yolo-World"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlRZIjWQ6hRR"
      },
      "outputs": [],
      "source": [
        "!wget https://huggingface.co/spaces/stevengrove/YOLO-World/resolve/main/yolow-v8_l_clipv2_frozen_t2iv2_bn_o365_goldg_pretrain.pth?download=true\n",
        "!mv yolow-v8_l_clipv2_frozen_t2iv2_bn_o365_goldg_pretrain.pth?download=true yolow-v8_l_clipv2_frozen_t2iv2_bn_o365_goldg_pretrain.pth\n",
        "!wget https://huggingface.co/spaces/stevengrove/YOLO-World/resolve/main/configs/pretrain/yolo_world_l_t2i_bn_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py?download=true\n",
        "!mv yolo_world_l_t2i_bn_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py?download=true yolo_world_l_t2i_bn_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41U-i5N06hRS"
      },
      "outputs": [],
      "source": [
        "!cp -r yolo_world_l_t2i_bn_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py /content/YOLO-World/configs/pretrain/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwP1HLoj6hRT"
      },
      "outputs": [],
      "source": [
        "import mmengine\n",
        "import yolo_world\n",
        "import mmyolo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wyGwF_QM6hRT"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os.path as osp\n",
        "from functools import partial\n",
        "import supervision as sv\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from tempfile import NamedTemporaryFile\n",
        "from PIL import Image\n",
        "from torchvision.ops import nms\n",
        "from mmengine.config import Config, DictAction\n",
        "from mmengine.runner import Runner\n",
        "from mmengine.runner.amp import autocast\n",
        "from mmengine.dataset import Compose\n",
        "from mmdet.visualization import DetLocalVisualizer\n",
        "from mmdet.datasets import CocoDataset\n",
        "from mmyolo.registry import RUNNERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vvFWXmC86hRU"
      },
      "outputs": [],
      "source": [
        "def setup_runner(cfg):\n",
        "\n",
        "    if 'runner_type' not in cfg:\n",
        "        runner = Runner.from_cfg(cfg)\n",
        "    else:\n",
        "        runner = RUNNERS.build(cfg)\n",
        "\n",
        "    runner.call_hook('before_run')\n",
        "    runner.load_or_resume()\n",
        "    pipeline = cfg.test_dataloader.dataset.pipeline\n",
        "    runner.pipeline = Compose(pipeline)\n",
        "    runner.model.eval()\n",
        "\n",
        "    bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
        "    label_annotator = sv.LabelAnnotator(text_color=sv.Color.BLACK)\n",
        "    return runner, bounding_box_annotator, label_annotator\n",
        "\n",
        "def run_image(\n",
        "    image: np.ndarray,\n",
        "    text,\n",
        "    cfg,\n",
        "    max_num_boxes = 100,\n",
        "    score_thr = 0.05,\n",
        "    nms_thr = 0.5\n",
        "):\n",
        "    runner, bounding_box_annotator, label_annotator = setup_runner(cfg)\n",
        "    with NamedTemporaryFile(suffix=\".jpeg\") as f:\n",
        "        cv2.imwrite(f.name, image)\n",
        "        texts = [[t.strip()] for t in text.split(',')] + [[' ']]\n",
        "        data_info = dict(img_id=0, img_path=f.name, texts=texts)\n",
        "        data_info = runner.pipeline(data_info)\n",
        "        data_batch = dict(inputs=data_info['inputs'].unsqueeze(0),\n",
        "                          data_samples=[data_info['data_samples']])\n",
        "\n",
        "        with autocast(enabled=False), torch.no_grad():\n",
        "            output = runner.model.test_step(data_batch)[0]\n",
        "            pred_instances = output.pred_instances\n",
        "\n",
        "        keep_idxs = nms(pred_instances.bboxes, pred_instances.scores, iou_threshold=nms_thr)\n",
        "\n",
        "        pred_instances = pred_instances[keep_idxs]\n",
        "        pred_instances = pred_instances[pred_instances.scores.float() > score_thr]\n",
        "\n",
        "        if len(pred_instances.scores) > max_num_boxes:\n",
        "            indices = pred_instances.scores.float().topk(max_num_boxes)[1]\n",
        "            pred_instances = pred_instances[indices]\n",
        "\n",
        "        pred_instances = pred_instances.cpu().numpy()\n",
        "\n",
        "        print(pred_instances['labels'])\n",
        "\n",
        "        detections = sv.Detections(\n",
        "            xyxy=pred_instances['bboxes'],\n",
        "            class_id=pred_instances['labels'],\n",
        "            confidence=pred_instances['scores'],\n",
        "            data={\n",
        "                'class_name': np.array([texts[class_id][0] for class_id in pred_instances['labels']])\n",
        "            }\n",
        "        )\n",
        "\n",
        "        labels = [\n",
        "            f\"{class_name} {confidence:0.2f}\"\n",
        "            for class_name, confidence\n",
        "            in zip(detections['class_name'], detections.confidence)\n",
        "        ]\n",
        "        annotated_image = image.copy()\n",
        "        annotated_image = bounding_box_annotator.annotate(annotated_image, detections)\n",
        "        annotated_image = label_annotator.annotate(annotated_image, detections, labels)\n",
        "        return annotated_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLn3OsMV6hRV"
      },
      "outputs": [],
      "source": [
        "cfg = Config.fromfile(\"/content/YOLO-World/configs/pretrain/yolo_world_l_t2i_bn_2e-4_100e_4x8gpus_obj365v1_goldg_train_lvis_minival.py\")\n",
        "cfg.work_dir = \".\"\n",
        "cfg.load_from = \"yolow-v8_l_clipv2_frozen_t2iv2_bn_o365_goldg_pretrain.pth\"\n",
        "# class_names = \"person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant, stop sign, parking meter, bench, bird, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe, backpack, umbrella, handbag, tie, suitcase, frisbee, skis, snowboard, sports ball, kite, baseball bat, baseball glove, skateboard, surfboard, tennis racket, bottle, wine glass, cup, fork, knife, spoon, bowl, banana, apple, sandwich, orange, broccoli, carrot, hot dog, pizza, donut, cake, chair, couch, potted plant, bed, dining table, toilet, tv, laptop, mouse, remote, keyboard, cell phone, microwave, oven, toaster, sink, refrigerator, book, clock, vase, scissors, teddy bear, hair drier, toothbrush\"\n",
        "\n",
        "class_names = \"dog, person, car, bike, bicycle, tree, hand, nose, hair\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJdp1OXm6hRV"
      },
      "outputs": [],
      "source": [
        "image = run_image(cv2.imread('/content/car-chase-featured.jpg') , class_names, cfg)\n",
        "sv.plot_image(image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYaHKaRd6hRW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZYN_LSe6hRW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sLOYZJKY6hRW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
